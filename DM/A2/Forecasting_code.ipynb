{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e480a5",
   "metadata": {},
   "source": [
    "# Sales Forecasting\n",
    "\n",
    "This model trains machine learning algorithms to perform sales forecasting. We consider XGBoost, LightGBM, and a blending model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16377866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:07:50.431798Z",
     "start_time": "2022-12-08T20:07:50.420990Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd \n",
    "import re\n",
    "from itertools import product\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ce3ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:07:29.940435Z",
     "start_time": "2022-12-08T20:07:28.366685Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "items=pd.read_csv(\"competitive-data-science-predict-future-sales/items.csv\")\n",
    "shops=pd.read_csv(\"competitive-data-science-predict-future-sales/shops.csv\")\n",
    "cats=pd.read_csv(\"competitive-data-science-predict-future-sales/item_categories.csv\")\n",
    "train=pd.read_csv(\"competitive-data-science-predict-future-sales/sales_train.csv\")\n",
    "test=pd.read_csv(\"competitive-data-science-predict-future-sales/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db06f67",
   "metadata": {},
   "source": [
    "We do some cleaning to the data, i.e remove outliers and clip the sales between 0 and 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243e1f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:07:32.934291Z",
     "start_time": "2022-12-08T20:07:32.203221Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop shops 9 and 20 from training set, they don't appear in the test set\n",
    "train = train.drop(labels = train[train.shop_id == 9].index, axis = 0)\n",
    "train = train.drop(labels = train[train.shop_id == 20].index, axis = 0)\n",
    "\n",
    "#get rid of obvious outliers\n",
    "train = train[(train.item_price < 300000 )& (train.item_cnt_day < 1000)]\n",
    "\n",
    "train.loc[train.item_cnt_day < 0, \"item_cnt_day\"] = 0 # or -train.loc[train.item_cnt_day < 0, \"item_cnt_day\"]\n",
    "train.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0\n",
    "\n",
    "train['item_cnt_day_unclipped'] = train['item_cnt_day']\n",
    "\n",
    "#We want to clip the target value before aggregating so that mean values are not distorted due to outliers. We retain the unclipped value for use in features that do not aggregate the sales data.\n",
    "train['item_cnt_day'] = train['item_cnt_day'].clip(0, 20)   #early clipping helps get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780f355",
   "metadata": {},
   "source": [
    "Check what we have in the test set. We output the occurence of the 3 item categories in the test set. This is discussed in Section 2.3 of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67944778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:07:34.333966Z",
     "start_time": "2022-12-08T20:07:33.470486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of good pairs: 111404\n",
      "2. No Data Items: 16086\n",
      "3. Only Item_id Info: 86710\n"
     ]
    }
   ],
   "source": [
    "good_sales = test.merge(train, on=['item_id','shop_id'], how='left').dropna()\n",
    "good_pairs = test[test['ID'].isin(good_sales['ID'])]   #so for these test sample we have training data\n",
    "no_data_items = test[~(test['item_id'].isin(train['item_id']))]   #these items are not in the training set\n",
    "\n",
    "print('1. Number of good pairs:', len(good_pairs))\n",
    "print('2. No Data Items:', len(no_data_items))\n",
    "print('3. Only Item_id Info:', len(test)-len(no_data_items)-len(good_pairs))  #for these the item is in the training set but the item/shop combination is not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e82f90",
   "metadata": {},
   "source": [
    "We are handling the shop duplicates now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f481e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:07:53.821184Z",
     "start_time": "2022-12-08T20:07:53.791067Z"
    }
   },
   "outputs": [],
   "source": [
    "#there are dublicate shops\n",
    "\n",
    "# Якутск Орджоникидзе, 56\n",
    "train.loc[train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train.loc[train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train.loc[train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee069721",
   "metadata": {},
   "source": [
    "We now extract properties like city, item/shop category and item names from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c380501a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:07:58.012700Z",
     "start_time": "2022-12-08T20:07:57.952069Z"
    }
   },
   "outputs": [],
   "source": [
    "#define some properties of the shops (city, category, name)\n",
    "shops.loc[ shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\n",
    "shops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\n",
    "shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "\n",
    "category = []\n",
    "for cat in shops.category.unique():\n",
    "    if len(shops[shops.category == cat]) >= 5:\n",
    "        category.append(cat)\n",
    "shops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )\n",
    "\n",
    "shops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\n",
    "shops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\n",
    "shops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n",
    "\n",
    "#define some properties of the items\n",
    "\n",
    "cats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\n",
    "cats.loc[ (cats.type_code == \"Игровые\")| (cats.type_code == \"Аксессуары\"), \"category\" ] = \"Игры\"\n",
    "\n",
    "category = []\n",
    "for cat in cats.type_code.unique():\n",
    "    if len(cats[cats.type_code == cat]) >= 5: \n",
    "        category.append( cat )\n",
    "cats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")\n",
    "\n",
    "cats.type_code = LabelEncoder().fit_transform(cats.type_code)\n",
    "cats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\n",
    "cats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "cats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\n",
    "cats = cats[[\"item_category_id\", \"subtype_code\", \"type_code\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a151f33",
   "metadata": {},
   "source": [
    "We are cleaning the item data, there are some inconsistencies in the string variables describing the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9998e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:08:00.182157Z",
     "start_time": "2022-12-08T20:08:00.175841Z"
    }
   },
   "outputs": [],
   "source": [
    "def name_correction(x):\n",
    "    x = x.lower() # all letters lower case\n",
    "    x = x.partition('[')[0] # partition by square brackets\n",
    "    x = x.partition('(')[0] # partition by curly brackets\n",
    "    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x) # remove special characters\n",
    "    x = x.replace('  ', ' ') # replace double spaces with single spaces\n",
    "    x = x.strip() # remove leading and trailing white space\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174b0204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:08:03.101388Z",
     "start_time": "2022-12-08T20:08:02.625285Z"
    }
   },
   "outputs": [],
   "source": [
    "# split item names by first bracket\n",
    "items[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\n",
    "items[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n",
    "\n",
    "# replace special characters and turn to lower case\n",
    "items[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n",
    "items[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n",
    "\n",
    "# fill nulls with '0'\n",
    "items = items.fillna('0')\n",
    "\n",
    "items[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n",
    "\n",
    "# return all characters except the last if name 2 is not \"0\" - the closing bracket\n",
    "items.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n",
    "\n",
    "\n",
    "\n",
    "items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\n",
    "items.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\n",
    "items.loc[ items.type == \"\", \"type\"] = \"mac\"\n",
    "items.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\n",
    "items.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\n",
    "items.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"\n",
    "\n",
    "\n",
    "group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\n",
    "group_sum = group_sum.reset_index()\n",
    "drop_cols = []\n",
    "for cat in group_sum.type.unique():\n",
    "    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n",
    "        drop_cols.append(cat)\n",
    "items.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\n",
    "items = items.drop([\"type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b175f",
   "metadata": {},
   "source": [
    "We now find that there are possible item duplicates in the data. For potential duplicates, we check if their item IDs are present in the test set. If both their IDs exist in the test\n",
    "set, we decide to leave the data unchanged. When only one of the IDs is in the test set, we assign\n",
    "the other item to this ID. By doing so, we reduce the cases of items in the test set for which we\n",
    "have no sales history to 7% (i.e by 630 items, see Section 2.3 in the report). If none of the IDs are in the test\n",
    "set, we assign both items to the same ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace32388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:09:05.270978Z",
     "start_time": "2022-12-08T20:08:05.888899Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/kxyb9__d7q11lb0tprf805cr0000gn/T/ipykernel_23139/4231260571.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupes['in_test'] = dupes.item_id.isin(test.item_id.unique())   #checks if one of the dublicate items is in the test set\n"
     ]
    }
   ],
   "source": [
    "#Duplicate rows exist in the item list. The following cell creates a dictionary that will allow us to reassign item id's where appropriate.\n",
    "\n",
    "dupes = items[(items.duplicated(subset=['item_name','item_category_id'],keep=False))]   #gets the dublicate rows\n",
    "\n",
    "\n",
    "dupes['in_test'] = dupes.item_id.isin(test.item_id.unique())   #checks if one of the dublicate items is in the test set\n",
    "\n",
    "dupes = dupes.groupby('item_name').agg({'item_id':['first','last'],'in_test':['first','last']})  #puts the dublicates in the same row\n",
    "\n",
    "#if both item id's are in the test set do nothing\n",
    "dupes = dupes[(dupes[('in_test', 'first')]==False) | (dupes[('in_test', 'last')]==False)]  #we only consider cases where one is not in the test set\n",
    "\n",
    "#if only the first id is in the test set assign this id to both\n",
    "temp = dupes[dupes[('in_test', 'first')]==True]\n",
    "keep_first = dict(zip(temp[('item_id', 'last')], temp[('item_id',  'first')]))\n",
    "#if neither id or only the second id is in the test set, assign the second id to both\n",
    "temp = dupes[dupes[('in_test', 'first')]==False]\n",
    "keep_second = dict(zip(temp[('item_id', 'first')], temp[('item_id',  'last')]))\n",
    "item_map = {**keep_first, **keep_second}\n",
    "\n",
    "train = (train.replace({'item_id': item_map}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1fc1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:09:22.735254Z",
     "start_time": "2022-12-08T20:09:16.050223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of good pairs: 113143\n",
      "2. No Data Items: 15456\n",
      "3. Only Item_id Info: 85601\n"
     ]
    }
   ],
   "source": [
    "good_sales = test.merge(train, on=['item_id','shop_id'], how='left').dropna()\n",
    "good_pairs = test[test['ID'].isin(good_sales['ID'])]   #so for these test sample we have training data\n",
    "no_data_items = test[~(test['item_id'].isin(train['item_id']))]   #these items are not in the training set\n",
    "\n",
    "print('1. Number of good pairs:', len(good_pairs))\n",
    "print('2. No Data Items:', len(no_data_items))\n",
    "print('3. Only Item_id Info:', len(test)-len(no_data_items)-len(good_pairs))  #for these the item is in the training set but the item/shop combination is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd882131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:09:24.210171Z",
     "start_time": "2022-12-08T20:09:23.811339Z"
    }
   },
   "outputs": [],
   "source": [
    "items.name2 = LabelEncoder().fit_transform(items.name2)\n",
    "items.name3 = LabelEncoder().fit_transform(items.name3)\n",
    "items.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17f2a7",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "We now perform the feature engineering process where we add properties to the data which are useful for the forecastin task. This is similar to what is being done in the other notebooks. The description of all the features is given in Table A1 in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c6db8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:09:38.230363Z",
     "start_time": "2022-12-08T20:09:25.080562Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = []\n",
    "cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\n",
    "for i in range(34):\n",
    "    sales = train[train.date_block_num == i]\n",
    "    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n",
    "\n",
    "matrix = pd.DataFrame( np.vstack(matrix), columns = cols )\n",
    "matrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\n",
    "matrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\n",
    "matrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\n",
    "matrix.sort_values( cols, inplace = True )\n",
    "time.time()- ts\n",
    "\n",
    "# add revenue to train df\n",
    "train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e916323a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:09:54.530537Z",
     "start_time": "2022-12-08T20:09:41.275503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.283644914627075"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\n",
    "group.columns = [\"item_cnt_month\"]\n",
    "group.reset_index( inplace = True)\n",
    "matrix = pd.merge( matrix, group, on = cols, how = \"left\" )\n",
    "matrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).astype(np.float16)\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "test[\"date_block_num\"] = 34\n",
    "test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "test[\"item_id\"] = test.item_id.astype(np.int16)\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "matrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\n",
    "matrix.fillna( 0, inplace = True )\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "matrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\n",
    "matrix = pd.merge( matrix, cats, on = [\"item_category_id\"], how = \"left\" )\n",
    "matrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\n",
    "matrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\n",
    "matrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\n",
    "matrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\n",
    "matrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c3fd32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:10:00.575658Z",
     "start_time": "2022-12-08T20:09:55.842127Z"
    }
   },
   "outputs": [],
   "source": [
    "#clustering shops\n",
    "shops_cats = pd.DataFrame(\n",
    "    np.array(list(product(*[train['shop_id'].unique(), matrix['item_category_id'].unique()]))),\n",
    "    columns =['shop_id', 'item_category_id']\n",
    ")\n",
    "temp = matrix.groupby(['item_category_id', 'shop_id']).agg({'item_cnt_month':'sum'}).reset_index()\n",
    "temp2 = temp.groupby('shop_id').agg({'item_cnt_month':'sum'}).rename(columns={'item_cnt_month':'shop_total'})\n",
    "temp = temp.join(temp2, on='shop_id')\n",
    "temp['category_proportion'] = temp['item_cnt_month']/temp['shop_total']\n",
    "temp = temp[['shop_id', 'item_category_id', 'category_proportion']]\n",
    "shops_cats = pd.merge(shops_cats, temp, on=['shop_id','item_category_id'], how='left')\n",
    "shops_cats = shops_cats.fillna(0)\n",
    "\n",
    "shops_cats = shops_cats.pivot(index='shop_id', columns=['item_category_id'])\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(shops_cats)\n",
    "shops_cats['shop_cluster'] = kmeans.labels_.astype('int8')\n",
    "\n",
    "#adding these clusters to the shops dataframe\n",
    "shops = shops.join(shops_cats['shop_cluster'], on='shop_id')\n",
    "\n",
    "\n",
    "matrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\n",
    "matrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\n",
    "matrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59d021a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T20:10:25.722338Z",
     "start_time": "2022-12-08T20:10:25.700688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a lag feature function\n",
    "def lag_feature( df,lags, cols ):\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n",
    "        for i in lags:\n",
    "            shifted = tmp.copy()\n",
    "            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n",
    "            shifted.date_block_num = shifted.date_block_num + i\n",
    "            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cdf16c4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T20:10:26.421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_month\n",
      "date_avg_item_cnt\n",
      "date_item_avg_item_cnt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/kxyb9__d7q11lb0tprf805cr0000gn/T/ipykernel_23139/2902880678.py:33: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  matrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_shop_avg_item_cnt\n",
      "date_shop_subtype_avg_item_cnt\n",
      "date_city_avg_item_cnt\n",
      "date_item_city_avg_item_cnt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.41569995880127"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
    "group.columns = [\"date_avg_item_cnt\"]\n",
    "group.reset_index(inplace = True)\n",
    "matrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\n",
    "matrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\n",
    "matrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\n",
    "matrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\n",
    "matrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
    "group.columns = [\"date_shop_avg_item_cnt\"]\n",
    "group.reset_index(inplace = True)\n",
    "matrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\n",
    "matrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\n",
    "matrix = lag_feature( matrix, [1,2,3,4], [\"date_shop_avg_item_cnt\"] )\n",
    "matrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_subtype_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\n",
    "matrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1, 2], ['date_shop_subtype_avg_item_cnt'])\n",
    "matrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_city_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', \"shop_city\"], how='left')\n",
    "matrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\n",
    "matrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_city_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\n",
    "matrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\n",
    "matrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603a28c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_item_avg_item_price\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.502816915512085"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\n",
    "group.columns = [\"item_avg_item_price\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\n",
    "matrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n",
    "\n",
    "\n",
    "group = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\n",
    "group.columns = [\"date_item_avg_item_price\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\n",
    "matrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\n",
    "lags = [1]\n",
    "matrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\n",
    "for i in lags:\n",
    "    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )/ matrix[\"item_avg_item_price\"]\n",
    "\n",
    "\n",
    "features_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\n",
    "for i in lags:\n",
    "    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n",
    "\n",
    "\n",
    "matrix.drop(features_to_drop, axis = 1, inplace = True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db3dfa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_revenue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.62042498588562"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\n",
    "group.columns = [\"date_shop_revenue\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\n",
    "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
    "\n",
    "group = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\n",
    "group.columns = [\"shop_avg_revenue\"]\n",
    "group.reset_index(inplace = True )\n",
    "\n",
    "matrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\n",
    "matrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\n",
    "matrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
    "matrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], [\"delta_revenue\"])\n",
    "matrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\n",
    "matrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce992d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.458292007446289"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[\"month\"] = matrix[\"date_block_num\"] % 12\n",
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)\n",
    "\n",
    "ts = time.time()\n",
    "matrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\n",
    "matrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n",
    "time.time() - ts\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "matrix = matrix[matrix[\"date_block_num\"] > 0] #> 0 gives best result\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d22c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix.to_csv('matrix_opt4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef3e24",
   "metadata": {},
   "source": [
    "### Modelling (Starting to train Models begins here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1b0d329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1445631</th>\n",
       "      <th>1445632</th>\n",
       "      <th>1445633</th>\n",
       "      <th>1445634</th>\n",
       "      <th>1445635</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date_block_num</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_id</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_cnt_month</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_category_id</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name2</th>\n",
       "      <td>76.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>123.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name3</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>562.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtype_code</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>16.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_code</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_category</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_city</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_cluster</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <td>0.296387</td>\n",
       "      <td>0.296387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.296387</td>\n",
       "      <td>0.296387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <td>0.021744</td>\n",
       "      <td>0.130493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.826172</td>\n",
       "      <td>1.260742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <td>0.086975</td>\n",
       "      <td>0.152222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.046875</td>\n",
       "      <td>4.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <td>0.065247</td>\n",
       "      <td>0.173950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.734375</td>\n",
       "      <td>13.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_avg_item_cnt_lag_1</th>\n",
       "      <td>0.074305</td>\n",
       "      <td>0.074305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074305</td>\n",
       "      <td>0.074305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_avg_item_cnt_lag_2</th>\n",
       "      <td>0.094606</td>\n",
       "      <td>0.094606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094606</td>\n",
       "      <td>0.094606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_avg_item_cnt_lag_3</th>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.061883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_avg_item_cnt_lag_4</th>\n",
       "      <td>0.146980</td>\n",
       "      <td>0.146980</td>\n",
       "      <td>0.14698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_shop_subtype_avg_item_cnt_lag_1</th>\n",
       "      <td>0.455078</td>\n",
       "      <td>0.387207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.034088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_city_avg_item_cnt_lag_1</th>\n",
       "      <td>0.074280</td>\n",
       "      <td>0.074280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074280</td>\n",
       "      <td>0.074280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_item_city_avg_item_cnt_lag_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delta_price_lag_1</th>\n",
       "      <td>0.367676</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212402</td>\n",
       "      <td>0.191040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <td>37326.816406</td>\n",
       "      <td>37326.816406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37326.816406</td>\n",
       "      <td>37326.816406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_shop_first_sale</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_first_sale</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           1445631       1445632    1445633  \\\n",
       "date_block_num                            4.000000      4.000000    4.00000   \n",
       "shop_id                                   2.000000      2.000000    2.00000   \n",
       "item_id                                  27.000000     28.000000   29.00000   \n",
       "item_cnt_month                            0.000000      0.000000    0.00000   \n",
       "item_category_id                         19.000000     30.000000   23.00000   \n",
       "name2                                    76.000000    107.000000  123.00000   \n",
       "name3                                    42.000000     42.000000   42.00000   \n",
       "subtype_code                             10.000000     55.000000   16.00000   \n",
       "type_code                                 3.000000      3.000000    3.00000   \n",
       "shop_category                             4.000000      4.000000    4.00000   \n",
       "shop_city                                 0.000000      0.000000    0.00000   \n",
       "shop_cluster                              5.000000      5.000000    5.00000   \n",
       "item_cnt_month_lag_1                      0.000000      0.000000        NaN   \n",
       "item_cnt_month_lag_2                      0.000000      0.000000        NaN   \n",
       "item_cnt_month_lag_3                      0.000000      0.000000        NaN   \n",
       "date_avg_item_cnt_lag_1                   0.296387      0.296387        NaN   \n",
       "date_item_avg_item_cnt_lag_1              0.021744      0.130493        NaN   \n",
       "date_item_avg_item_cnt_lag_2              0.086975      0.152222        NaN   \n",
       "date_item_avg_item_cnt_lag_3              0.065247      0.173950        NaN   \n",
       "date_shop_avg_item_cnt_lag_1              0.074305      0.074305        NaN   \n",
       "date_shop_avg_item_cnt_lag_2              0.094606      0.094606        NaN   \n",
       "date_shop_avg_item_cnt_lag_3              0.061883      0.061883        NaN   \n",
       "date_shop_avg_item_cnt_lag_4              0.146980      0.146980    0.14698   \n",
       "date_shop_subtype_avg_item_cnt_lag_1      0.455078      0.387207        NaN   \n",
       "date_city_avg_item_cnt_lag_1              0.074280      0.074280        NaN   \n",
       "date_item_city_avg_item_cnt_lag_1         0.000000      0.000000        NaN   \n",
       "delta_price_lag_1                         0.367676      0.256348        NaN   \n",
       "delta_revenue_lag_1                   37326.816406  37326.816406        NaN   \n",
       "month                                     4.000000      4.000000    4.00000   \n",
       "days                                     31.000000     31.000000   31.00000   \n",
       "item_shop_first_sale                      4.000000      4.000000    4.00000   \n",
       "item_first_sale                           4.000000      4.000000    4.00000   \n",
       "\n",
       "                                           1445634       1445635  \n",
       "date_block_num                            4.000000      4.000000  \n",
       "shop_id                                   2.000000      2.000000  \n",
       "item_id                                  30.000000     31.000000  \n",
       "item_cnt_month                            0.000000      0.000000  \n",
       "item_category_id                         40.000000     37.000000  \n",
       "name2                                     4.000000      4.000000  \n",
       "name3                                    42.000000    562.000000  \n",
       "subtype_code                              4.000000      1.000000  \n",
       "type_code                                 5.000000      5.000000  \n",
       "shop_category                             4.000000      4.000000  \n",
       "shop_city                                 0.000000      0.000000  \n",
       "shop_cluster                              5.000000      5.000000  \n",
       "item_cnt_month_lag_1                      0.000000      1.000000  \n",
       "item_cnt_month_lag_2                      1.000000      1.000000  \n",
       "item_cnt_month_lag_3                      0.000000      4.000000  \n",
       "date_avg_item_cnt_lag_1                   0.296387      0.296387  \n",
       "date_item_avg_item_cnt_lag_1              2.826172      1.260742  \n",
       "date_item_avg_item_cnt_lag_2             11.046875      4.781250  \n",
       "date_item_avg_item_cnt_lag_3             18.734375     13.648438  \n",
       "date_shop_avg_item_cnt_lag_1              0.074305      0.074305  \n",
       "date_shop_avg_item_cnt_lag_2              0.094606      0.094606  \n",
       "date_shop_avg_item_cnt_lag_3              0.061883      0.061883  \n",
       "date_shop_avg_item_cnt_lag_4                   NaN           NaN  \n",
       "date_shop_subtype_avg_item_cnt_lag_1      0.020538      0.034088  \n",
       "date_city_avg_item_cnt_lag_1              0.074280      0.074280  \n",
       "date_item_city_avg_item_cnt_lag_1         0.000000      1.000000  \n",
       "delta_price_lag_1                         0.212402      0.191040  \n",
       "delta_revenue_lag_1                   37326.816406  37326.816406  \n",
       "month                                     4.000000      4.000000  \n",
       "days                                     31.000000     31.000000  \n",
       "item_shop_first_sale                      3.000000      3.000000  \n",
       "item_first_sale                           3.000000      3.000000  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = pd.read_csv('matrix_opt4.csv', index_col = 0)\n",
    "matrix.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad478de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training, validation and test set\n",
    "import gc\n",
    "data = matrix.copy()\n",
    "del matrix\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1).fillna(0)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month'].fillna(0)\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1).fillna(0)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month'].fillna(0)\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1).fillna(0)\n",
    "\n",
    "Y_train = Y_train.clip(0, 20)\n",
    "Y_valid = Y_valid.clip(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9921196",
   "metadata": {},
   "source": [
    "#### XGBoost Model \n",
    "\n",
    "Here we train our XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72847d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ff9b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louissiebenaler/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "/Users/louissiebenaler/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1.21494\tvalidation_1-rmse:1.08808\n",
      "[1]\tvalidation_0-rmse:1.18236\tvalidation_1-rmse:1.06828\n",
      "[2]\tvalidation_0-rmse:1.15125\tvalidation_1-rmse:1.05010\n",
      "[3]\tvalidation_0-rmse:1.12148\tvalidation_1-rmse:1.03303\n",
      "[4]\tvalidation_0-rmse:1.09306\tvalidation_1-rmse:1.01696\n",
      "[5]\tvalidation_0-rmse:1.06586\tvalidation_1-rmse:1.00148\n",
      "[6]\tvalidation_0-rmse:1.03982\tvalidation_1-rmse:0.98725\n",
      "[7]\tvalidation_0-rmse:1.01487\tvalidation_1-rmse:0.97433\n",
      "[8]\tvalidation_0-rmse:0.99108\tvalidation_1-rmse:0.96235\n",
      "[9]\tvalidation_0-rmse:0.96823\tvalidation_1-rmse:0.95113\n",
      "[10]\tvalidation_0-rmse:0.94624\tvalidation_1-rmse:0.93971\n",
      "[11]\tvalidation_0-rmse:0.92527\tvalidation_1-rmse:0.92912\n",
      "[12]\tvalidation_0-rmse:0.90528\tvalidation_1-rmse:0.91915\n",
      "[13]\tvalidation_0-rmse:0.88600\tvalidation_1-rmse:0.91003\n",
      "[14]\tvalidation_0-rmse:0.86761\tvalidation_1-rmse:0.90191\n",
      "[15]\tvalidation_0-rmse:0.85004\tvalidation_1-rmse:0.89414\n",
      "[16]\tvalidation_0-rmse:0.83328\tvalidation_1-rmse:0.88744\n",
      "[17]\tvalidation_0-rmse:0.81724\tvalidation_1-rmse:0.88124\n",
      "[18]\tvalidation_0-rmse:0.80186\tvalidation_1-rmse:0.87540\n",
      "[19]\tvalidation_0-rmse:0.78717\tvalidation_1-rmse:0.87015\n",
      "[20]\tvalidation_0-rmse:0.77315\tvalidation_1-rmse:0.86540\n",
      "[21]\tvalidation_0-rmse:0.75982\tvalidation_1-rmse:0.86113\n",
      "[22]\tvalidation_0-rmse:0.74679\tvalidation_1-rmse:0.85682\n",
      "[23]\tvalidation_0-rmse:0.73449\tvalidation_1-rmse:0.85271\n",
      "[24]\tvalidation_0-rmse:0.72282\tvalidation_1-rmse:0.84815\n",
      "[25]\tvalidation_0-rmse:0.71145\tvalidation_1-rmse:0.84489\n",
      "[26]\tvalidation_0-rmse:0.70069\tvalidation_1-rmse:0.84153\n",
      "[27]\tvalidation_0-rmse:0.69044\tvalidation_1-rmse:0.83895\n",
      "[28]\tvalidation_0-rmse:0.68048\tvalidation_1-rmse:0.83688\n",
      "[29]\tvalidation_0-rmse:0.67105\tvalidation_1-rmse:0.83467\n",
      "[30]\tvalidation_0-rmse:0.66207\tvalidation_1-rmse:0.83287\n",
      "[31]\tvalidation_0-rmse:0.65327\tvalidation_1-rmse:0.83120\n",
      "[32]\tvalidation_0-rmse:0.64508\tvalidation_1-rmse:0.82949\n",
      "[33]\tvalidation_0-rmse:0.63714\tvalidation_1-rmse:0.82774\n",
      "[34]\tvalidation_0-rmse:0.62935\tvalidation_1-rmse:0.82585\n",
      "[35]\tvalidation_0-rmse:0.62192\tvalidation_1-rmse:0.82477\n",
      "[36]\tvalidation_0-rmse:0.61472\tvalidation_1-rmse:0.82300\n",
      "[37]\tvalidation_0-rmse:0.60793\tvalidation_1-rmse:0.82173\n",
      "[38]\tvalidation_0-rmse:0.60145\tvalidation_1-rmse:0.82110\n",
      "[39]\tvalidation_0-rmse:0.59493\tvalidation_1-rmse:0.82053\n",
      "[40]\tvalidation_0-rmse:0.58846\tvalidation_1-rmse:0.82009\n",
      "[41]\tvalidation_0-rmse:0.58276\tvalidation_1-rmse:0.81994\n",
      "[42]\tvalidation_0-rmse:0.57686\tvalidation_1-rmse:0.81974\n",
      "[43]\tvalidation_0-rmse:0.57119\tvalidation_1-rmse:0.81948\n",
      "[44]\tvalidation_0-rmse:0.56579\tvalidation_1-rmse:0.81968\n",
      "[45]\tvalidation_0-rmse:0.56073\tvalidation_1-rmse:0.81983\n",
      "[46]\tvalidation_0-rmse:0.55572\tvalidation_1-rmse:0.82028\n",
      "[47]\tvalidation_0-rmse:0.55103\tvalidation_1-rmse:0.82069\n",
      "[48]\tvalidation_0-rmse:0.54653\tvalidation_1-rmse:0.82138\n",
      "[49]\tvalidation_0-rmse:0.54174\tvalidation_1-rmse:0.82202\n",
      "[50]\tvalidation_0-rmse:0.53752\tvalidation_1-rmse:0.82279\n",
      "[51]\tvalidation_0-rmse:0.53314\tvalidation_1-rmse:0.82325\n",
      "[52]\tvalidation_0-rmse:0.52914\tvalidation_1-rmse:0.82373\n",
      "[53]\tvalidation_0-rmse:0.52512\tvalidation_1-rmse:0.82397\n",
      "[54]\tvalidation_0-rmse:0.52150\tvalidation_1-rmse:0.82447\n",
      "[55]\tvalidation_0-rmse:0.51794\tvalidation_1-rmse:0.82479\n",
      "[56]\tvalidation_0-rmse:0.51451\tvalidation_1-rmse:0.82530\n",
      "[57]\tvalidation_0-rmse:0.51102\tvalidation_1-rmse:0.82553\n",
      "[58]\tvalidation_0-rmse:0.50761\tvalidation_1-rmse:0.82604\n",
      "[59]\tvalidation_0-rmse:0.50476\tvalidation_1-rmse:0.82621\n",
      "[60]\tvalidation_0-rmse:0.50175\tvalidation_1-rmse:0.82655\n",
      "[61]\tvalidation_0-rmse:0.49878\tvalidation_1-rmse:0.82672\n",
      "[62]\tvalidation_0-rmse:0.49611\tvalidation_1-rmse:0.82699\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False, eta=0.04,\n",
       "             eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "             grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "             interaction_constraints=&#x27;&#x27;, learning_rate=0.0399999991,\n",
       "             max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=20, max_leaves=0,\n",
       "             min_child_weight=0.001, missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "             n_estimators=1100, n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False, eta=0.04,\n",
       "             eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "             grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "             interaction_constraints=&#x27;&#x27;, learning_rate=0.0399999991,\n",
       "             max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=20, max_leaves=0,\n",
       "             min_child_weight=0.001, missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "             n_estimators=1100, n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False, eta=0.04,\n",
       "             eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "             grow_policy='depthwise', importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.0399999991,\n",
       "             max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=20, max_leaves=0,\n",
       "             min_child_weight=0.001, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1100, n_jobs=0, num_parallel_tree=1, predictor='auto', ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ts = time.time()\n",
    "\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=10,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=0.5, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.1,\n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 20)\n",
    "\n",
    "#time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f01b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77816284, 1.4667418 , 1.8849636 , ..., 0.11341064, 0.09529134,\n",
       "       0.1393329 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performing the prediction on the test set\n",
    "Y_test = model.predict(X_test).clip(0, 20)\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5974d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the prediction\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('submission_xgb_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c322eb",
   "metadata": {},
   "source": [
    "#### LightGBM Model\n",
    "\n",
    "Here we train our LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd00576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val)\n",
    "    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=10,\n",
    "                     categorical_feature=cat_features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a830e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_rows', 160)\n",
    "pd.set_option('display.max_columns', 160)\n",
    "pd.set_option('display.max_colwidth', 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'rmse',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 1100,\n",
    "    'min_data_in_leaf':10,\n",
    "    'feature_fraction':0.7,\n",
    "    'learning_rate': 0.04, \n",
    "    'num_rounds': 1000,\n",
    "    'early_stopping_rounds': 20,\n",
    "    'seed': 1\n",
    "}\n",
    "\n",
    "#designating the categorical features which should be focused on\n",
    "cat_features = ['item_category_id','month','shop_id','shop_city']\n",
    "\n",
    "lgb_model = build_lgb_model(params, X_train, X_valid, Y_train, Y_valid, cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the prediction\n",
    "\n",
    "Y_test = lgb_model.predict(X_test).clip(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2530aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the prediction\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "#submission.to_csv('submission_lightgbm_opt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6633721",
   "metadata": {},
   "source": [
    "### Blending model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d0b2d",
   "metadata": {},
   "source": [
    "You basically have most of the code for this actually. After you run the XGBoost and lightGBM models, you want to add the predictions to a list such that you stack them i.e. append the XGBoost predictions, then the lightGBM or vice-versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for blending\n",
    "Y_train_pred = []\n",
    "Y_test_pred  = []\n",
    "Y_val_pred   = []\n",
    "\n",
    "# ------ Run LightGBM Model ------\n",
    "\n",
    "# LightGBM predictions\n",
    "Y_train_lgb = lgb_model.predict(X_train).clip(0,20)\n",
    "Y_test_lgb  = lgb_model.predict(X_test).clip(0,20)\n",
    "Y_val_lgb   = lgb_model.predict(X_valid).clip(0,20)\n",
    "\n",
    "# Add predictions to results list\n",
    "Y_train_pred.append(Y_train_lgb)\n",
    "Y_test_pred.append(Y_test_lgb)\n",
    "Y_val_pred.append(Y_val_lgb)\n",
    "\n",
    "# ------ Run XGBoost Model ------\n",
    "\n",
    "# XGBoost predictions\n",
    "Y_train_xgb = xgb_model.predict(X_train).clip(0, 20)\n",
    "Y_test_xgb  = xgb_model.predict(X_test).clip(0, 20)\n",
    "Y_val_xgb   = xgb_model.predict(X_valid).clip(0, 20)\n",
    "\n",
    "# Add predictions to results list\n",
    "Y_train_pred.append(Y_train_xgb)\n",
    "Y_test_pred.append(Y_test_xgb)\n",
    "Y_val_pred.append(Y_val_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efae7ec",
   "metadata": {},
   "source": [
    "### Blending\n",
    "This consists of combining different models using a so-called \"meta-model\". The meta-model is trained on the outputs of our XGBoost and LightGBM models to obtain a final prediction that may improve upon the individual predictions made by our  models.\n",
    "\n",
    "The general procedure when creating a blending model is to extract a holdout sample from the data to use as the validation set. It is the predictions made on the validation sets that are fed into the meta-model as input for training (i.e. holdout predictions are used as training data for the meta-model). The final predictions are then computed using the test predictions made by the XGBoost and LightGBM models as input for the meta-model.\n",
    "\n",
    "However, in the following we train the meta-model on the entire training set rather than the holdout set only as the best results were found in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "meta_model = LinearRegression() # this will be the regressor for our meta-model (worked best out of those tried)\n",
    "\n",
    "def blending_model(meta_model, train_preds, val_preds, test_preds, Y_train, Y_val):\n",
    "    stacked_train_pred = np.column_stack(train_preds)\n",
    "    stacked_test_pred  = np.column_stack(test_preds)\n",
    "    stacked_val_pred   = np.column_stack(val_preds)\n",
    "    \n",
    "    # Fit meta model on stacked predictions\n",
    "    meta_model.fit(stacked_train_pred, Y_train)\n",
    "    print('RMSE =', mean_squared_error(meta_model.predict(stacked_val_pred).clip(0, 20), Y_val, squared=False))\n",
    "    return meta_model.predict(stacked_test_pred).clip(0, 20)\n",
    "\n",
    "\n",
    "blending_Y_test = blending_model(meta_model, Y_train_pred, Y_val_pred, Y_test_pred, Y_train, Y_valid)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": test.index, \n",
    "#     \"item_cnt_month\": blending_Y_test\n",
    "# })\n",
    "# submission.to_csv('blending_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251793c",
   "metadata": {},
   "source": [
    "I'll put everything in the cell below (including the models) in case there's any confusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11dbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# Features\n",
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1).fillna(0)\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1).fillna(0)\n",
    "\n",
    "# Targets\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month'].fillna(0)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month'].fillna(0)\n",
    "\n",
    "# Test data\n",
    "X_test  = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1).fillna(0)\n",
    "\n",
    "# Clipping to keep outliers out of computations\n",
    "Y_train = Y_train.clip(0, 20)\n",
    "Y_valid = Y_valid.clip(0, 20)\n",
    "\n",
    "# Results for blending\n",
    "Y_train_pred = []\n",
    "Y_test_pred  = []\n",
    "Y_val_pred   = []\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# LightGBM Model\n",
    "params = {\n",
    "    'objective': 'rmse',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 1100,\n",
    "    'min_data_in_leaf':10,\n",
    "    'feature_fraction':0.7,\n",
    "    'learning_rate': 0.04, \n",
    "    'num_rounds': 1000,\n",
    "    'early_stopping_rounds': 20,\n",
    "    'seed': 1\n",
    "}\n",
    "\n",
    "\n",
    "# Categorical features to focus on\n",
    "cat_features = ['item_category_id', 'month', 'shop_id', 'shop_city']\n",
    "\n",
    "# LGB train and valid dataset\n",
    "lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "lgb_val   = lgb.Dataset(X_valid, Y_valid)\n",
    " \n",
    "# Train LightGBM model\n",
    "lgb_model = lgb.train(params=params,\n",
    "                      train_set=lgb_train,\n",
    "                      valid_sets=(lgb_train, lgb_val),\n",
    "                      verbose_eval=10,\n",
    "                      categorical_feature=cat_features)  \n",
    "\n",
    "# LightGBM predictions\n",
    "Y_train_lgb = lgb_model.predict(X_train).clip(0,20)\n",
    "Y_test_lgb  = lgb_model.predict(X_test).clip(0,20)\n",
    "Y_val_lgb   = lgb_model.predict(X_valid).clip(0,20)\n",
    "\n",
    "# Add predictions to results list\n",
    "Y_train_pred.append(Y_train_lgb)\n",
    "Y_test_pred.append(Y_test_lgb)\n",
    "Y_val_pred.append(Y_val_lgb)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBRegressor(\n",
    "    max_depth=10,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=0.5, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.1,\n",
    "    seed=42,\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=20)\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True)\n",
    "\n",
    "# XGBoost predictions\n",
    "Y_train_xgb = xgb_model.predict(X_train).clip(0, 20)\n",
    "Y_test_xgb  = xgb_model.predict(X_test).clip(0, 20)\n",
    "Y_val_xgb   = xgb_model.predict(X_valid).clip(0, 20)\n",
    "\n",
    "# Add predictions to results list\n",
    "Y_train_pred.append(Y_train_xgb)\n",
    "Y_test_pred.append(Y_test_xgb)\n",
    "Y_val_pred.append(Y_val_xgb)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# Blending model\n",
    "meta_model = LinearRegression() # this will be the regressor for our meta-model\n",
    "\n",
    "def blending_model(meta_model, train_preds, val_preds, test_preds, Y_train, Y_val):\n",
    "    stacked_train_pred = np.column_stack(train_preds)\n",
    "    stacked_test_pred  = np.column_stack(test_preds)\n",
    "    stacked_val_pred   = np.column_stack(val_preds)\n",
    "    \n",
    "    # Fit meta model on stacked predictions\n",
    "    meta_model.fit(stacked_train_pred, Y_train)\n",
    "    print('RMSE =', mean_squared_error(meta_model.predict(stacked_val_pred).clip(0, 20), Y_val, squared=False))\n",
    "    return meta_model.predict(stacked_test_pred).clip(0, 20)\n",
    "\n",
    "\n",
    "blending_Y_test = blending_model(meta_model, Y_train_pred, Y_val_pred, Y_test_pred, Y_train, Y_valid)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": test.index, \n",
    "#     \"item_cnt_month\": blending_Y_test\n",
    "# })\n",
    "# submission.to_csv('blending_model.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
