{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95aa3ddb",
   "metadata": {},
   "source": [
    "# Task 1 - Learn the basics of Keras and TensorFlow\n",
    "\n",
    "Note that two different persons have worked on this task. The part for MLP's was performed by one person and the part for CNN's was performed by the other. Hence there are slight differences in the coding for implementing the exploration of the hyperparameter idea but the same conceptuality applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fad69",
   "metadata": {},
   "source": [
    "### MLP's\n",
    "Here we studied the hyperparameters for MLP's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b776fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters we will test:\n",
    "# this function returns the values we will use for our reference model\n",
    "def get_default_params():\n",
    "\n",
    "  nlay = 3                                      # number of layers\n",
    "  k_reg = 'none'                                # kernel_regularizer\n",
    "  b_reg = 'none'                                # bias_regularizer\n",
    "  width = [100,30,10]                           # layer width\n",
    "  act_fun = ['relu', 'relu', 'softmax']         # activation functions\n",
    "  weights = 'glorot_uniform'                    # weights\n",
    "  dropout = 0                                   # dropout rate\n",
    "  loss = 'sparse_categorical_crossentropy'      # loss function\n",
    "  opt = 'sgd'                                   # optimizer\n",
    "  n_epochs = 40                                 # number of epochs\n",
    "  bs = 32                                       # batch size\n",
    "\n",
    "  return nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a756187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create and train our MLP\n",
    "class MLP(object):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "\n",
    "  # Function to collect the MNIST or fashion MNIST data\n",
    "  def load_data(self, data, loss):\n",
    "    (x_train, y_train), (x_test, y_test) = data.load_data()\n",
    "    \n",
    "    # Scaling the pixel intensities\n",
    "    x_train, x_test = x_train / 255, x_test / 255\n",
    "\n",
    "    # some loss functions require the y data to be categorical (one-hot encoding) \n",
    "    if loss != 'sparse_categorical_crossentropy':\n",
    "      y_train = to_categorical(y_train)\n",
    "      y_test = to_categorical(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "  # Function to create a custom MLP\n",
    "  def create_mlp(self, nlay, k_reg, b_reg, width, act_fun, weights, dropout):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=[28,28]))\n",
    "    \n",
    "    # Not including output layer so that dropout is added after last hidden layer only\n",
    "    for i in range(nlay-1):\n",
    "        model.add(layers.Dense(width[i], activation=act_fun[i],\n",
    "                  kernel_initializer=weights))\n",
    "\n",
    "    # Adding dropout to last hidden layer    \n",
    "    model.add(Dropout(dropout))  \n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(width[-1], activation=act_fun[-1],\n",
    "                            kernel_initializer=weights))\n",
    "\n",
    "    # Kernel regularizer\n",
    "    if k_reg != 'none':\n",
    "      for i,j in enumerate(k_reg):\n",
    "        model.layers[i].kernel_regularizer = j\n",
    "\n",
    "    # Bias regularizer\n",
    "    if b_reg != 'none':\n",
    "      for i,j in enumerate(b_reg):\n",
    "        model.layers[i].bias_initializer = 'ones'\n",
    "        model.layers[i].bias_regularizer = j\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "  # Function to compile our custom MLP\n",
    "  def compile_model(self, model, loss, optimizer):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "  # Function to train our custom MLP\n",
    "  def train_model(self, model, x_train, y_train, n_epochs, bs):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train, epochs=n_epochs,\n",
    "                    validation_split=0.2, batch_size=bs, \n",
    "                    callbacks=[early_stop])\n",
    "    print('Runtime: %s seconds' % (time.time() - start_time))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883404d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct, train, and save our custom MLP model \n",
    "def make_model(filename, nlay, k_reg, b_reg, width, act_fun, weights,\n",
    "               dropout, loss, opt, n_epochs, bs, dataset, save=True):\n",
    "  global early_stop\n",
    "\n",
    "  # Dataset will be either the MNIST or fashion MNIST\n",
    "  if dataset == 'digits':\n",
    "    data = keras.datasets.mnist\n",
    "  elif dataset == 'fashion':\n",
    "    data = keras.datasets.fashion_mnist\n",
    "  else:\n",
    "    print('dataset must be \"digits\" or \"fashion\"')\n",
    "    sys.exit()\n",
    "\n",
    "  # Stopping criterion \n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "  # Running previous codes\n",
    "  mlp = MLP()\n",
    "  X_train, Y_train, X_test, Y_test = mlp.load_data(data, loss)\n",
    "  model = mlp.create_mlp(nlay, k_reg, b_reg, width, act_fun, weights, dropout)\n",
    "  model = mlp.compile_model(model, loss, opt)\n",
    "  history = mlp.train_model(model, X_train, Y_train, n_epochs, bs)\n",
    "\n",
    "  # Saving\n",
    "  if save==True:\n",
    "    savepath_d = '/content/drive/MyDrive/Colab Notebooks/savefiles_mnist_digits/'\n",
    "    savepath_f = '/content/drive/MyDrive/Colab Notebooks/savefiles_mnist_fashion/'\n",
    " \n",
    "    if dataset == 'digits':\n",
    "      savepath = savepath_d\n",
    "    else:\n",
    "      savepath = savepath_f  \n",
    "\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    with open(savepath + filename + '.csv', mode='w') as f:\n",
    "      hist_df.to_csv(f)\n",
    "    \n",
    "    # Test case performance\n",
    "    test_case = np.array(model.evaluate(X_test, Y_test))\n",
    "    np.save(savepath + filename + '.npy', test_case)\n",
    "    with open(filename + '.npy', 'wb') as f:\n",
    "      np.save(f, test_case)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions to make our model for a range of values for each parameters\n",
    "\n",
    "# Reference model\n",
    "def test_reference(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  filename = 'defaults'\n",
    "  print(filename)\n",
    "  model = make_model(filename=filename, dataset=data, \n",
    "                      nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                      act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                      loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)\n",
    "     \n",
    "# Weights\n",
    "def test_weights(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  weights = ['zeros', 'random_normal', 'ones']\n",
    "  filename = ['weights_zeros', 'weights_rand', 'weights_ones']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                        act_fun=act_fun, weights=weights[i], dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)\n",
    "     \n",
    "# Dropouts\n",
    "def test_dropouts(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  dropout = [0.2, 0.5, 0.9]\n",
    "  filename = ['dropout02', 'dropout05', 'dropout09']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout[i], \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)\n",
    "    \n",
    "# Optimizers    \n",
    "def test_optimizer(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  opt = ['RMSprop', 'adam', 'nadam']\n",
    "  filename = ['opti_RMSprop', 'opti_adam', 'opti_nadam']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt[i], n_epochs=n_epochs, bs=bs)  \n",
    "\n",
    "# Kernel regularizer    \n",
    "def test_k_reg(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  k_reg = ['l1', 'l2']\n",
    "  filename = ['kreg_l1', 'kreg_l2']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg[i], b_reg=b_reg, width=width, \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)  \n",
    "    \n",
    "# Bias regularizer \n",
    "def test_b_reg(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  b_reg = ['l1', 'l2']\n",
    "  filename = ['breg_l1', 'breg_l2']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg[i], width=width, \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)   \n",
    "\n",
    "# Loss function\n",
    "def test_loss(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  loss = ['categorical_crossentropy', 'poisson', 'kullback_leibler_divergence']\n",
    "  filename = ['loss_cat', 'loss_poisson', 'loss_kl']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                        loss=loss[i], opt=opt, n_epochs=n_epochs, bs=bs)   \n",
    "\n",
    "# Batch Size\n",
    "def test_bs(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  bs = [100, 500, 1000]\n",
    "  filename = ['bs_100', 'bs_500', 'bs_1000']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs[i])\n",
    "\n",
    "\n",
    "# Widths\n",
    "def test_widths(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "  widths = [[50,50,50], [10,30,100], [500,250,50]]\n",
    "  filename = ['width_50_50_50', 'width_10_30_100', 'width_500_250_50']\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=widths[i], \n",
    "                        act_fun=act_fun, weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)    \n",
    "\n",
    "# Activation Functions\n",
    "def test_act_fun(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "\n",
    "  act_funs = [['relu', 'relu', 'softplus'], ['relu', 'relu', 'sigmoid'], \\\n",
    "              ['relu', 'relu', 'exponential']]\n",
    "  filename = ['act_plus', 'act_sig', 'act_exp']\n",
    "\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay, k_reg=k_reg, b_reg=b_reg, width=width, \n",
    "                        act_fun=act_funs[i], weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)  \n",
    "\n",
    "# Number of layers\n",
    "def test_nlay(data):\n",
    "  nlay, k_reg, b_reg, width, act_fun, weights, dropout, loss, opt, \\\n",
    "  n_epochs, bs = get_default_params()\n",
    "\n",
    "  nlay = [1, 10, 100]\n",
    "  w1 = [10]\n",
    "  w2 = [10]*10\n",
    "  w3 = [10]*100\n",
    "  widths = [w1, w2, w3]\n",
    "\n",
    "  a1 = ['softmax']\n",
    "  a2 = ['relu']*9\n",
    "  a2.append('softmax')\n",
    "  a3 = ['relu']*99\n",
    "  a3.append('softmax')\n",
    "\n",
    "  act_funs = [a1, a2, a3]\n",
    "  filename = ['nlay_1', 'nlay_10', 'nlay_100']\n",
    "\n",
    "  for i in range(len(filename)):\n",
    "    print(filename[i])\n",
    "    model = make_model(filename=filename[i], dataset=data, \n",
    "                        nlay=nlay[i], k_reg=k_reg, b_reg=b_reg, width=widths[i], \n",
    "                        act_fun=act_funs[i], weights=weights, dropout=dropout, \n",
    "                        loss=loss, opt=opt, n_epochs=n_epochs, bs=bs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running our code for both the MNIST and Fashion MNIST datasets\n",
    "def run_tests():\n",
    "\n",
    "  d = ['digits', 'fashion']\n",
    "\n",
    "  for i in range(len(d)):  \n",
    "    test_reference(d[i])\n",
    "    test_weights(d[i])\n",
    "    test_dropouts(d[i])\n",
    "    test_optimizer(d[i])\n",
    "    test_k_reg(d[i])\n",
    "    test_b_reg(d[i])\n",
    "    test_loss(d[i])\n",
    "    test_bs(d[i])\n",
    "    test_widths(d[i])\n",
    "    test_act_fun(d[i])\n",
    "    test_nlay(d[i])\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40b33a",
   "metadata": {},
   "source": [
    "### CNN's\n",
    "Here we studied the hyperparameters for CNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa206bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions to create, train CNN's\n",
    "\n",
    "#load up the data and normalise it \n",
    "def load_data(data):\n",
    "    (x_train, y_train), (x_test, y_test) = data.load_data()\n",
    "    x_train, x_test = x_train / 255, x_test / 255\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "#compile the model\n",
    "def compile_model(model, loss = \"sparse_categorical_crossentropy\", optimizer = 'sgd'):\n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "#train the model\n",
    "def train_model(model, x_train, y_train, num_epochs = 20, val_split = 0.2, bs = 32):\n",
    "    history = model.fit(x_train, y_train, epochs = num_epochs,\n",
    "                    validation_split = val_split, batch_size = bs, callbacks = [early_stop])\n",
    "    return history\n",
    "\n",
    "#make the appropraite for a CNN\n",
    "def cnn_data(x_train, x_test, dim = 28):\n",
    "    x_train = x_train.reshape(x_train.shape[0], dim, dim, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], dim, dim, 1)\n",
    "    return x_train, x_test\n",
    "\n",
    "#create a CNN\n",
    "def create_cnn(in_shape = [28, 28, 1], nclay = 7, nlay = 3, \n",
    "              carch = ['maxp', 'conv', 'conv', 'maxp', 'conv', 'conv', 'maxp'],\n",
    "              f = [64, 128, 128, 256, 256],\n",
    "              ps = 2, ks = 3, width = [128, 64, 10],\n",
    "              act_fun = ['relu', 'relu', 'softmax'],\n",
    "              dropout = 0.5):\n",
    "    #Meaning of the parameters:\n",
    "    #in_shape = input shape, nclay = number of hidden convolutional layers, nlay = number of hidden dense layers\n",
    "    #carch = order of convolutional and pooling layers in the model, f = number of filters fro each layer\n",
    "    #ps = pooling size, ks = kernel size, width = number of neurones for the dense layers\n",
    "    #act_fun = activation function for the layers ,dropout = dropout\n",
    "    \n",
    "    DefaultConv2D = partial(layers.Conv2D,\n",
    "                            kernel_size = ks, activation='relu', padding=\"SAME\")\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    c_counter = 0\n",
    "    mp_counter = 0\n",
    "    \n",
    "    model.add(DefaultConv2D(filters = f[c_counter], kernel_size = 7, \n",
    "                                    input_shape = in_shape, padding = 'same'))\n",
    "    c_counter += 1\n",
    "    for l in carch:\n",
    "        if l == 'conv':\n",
    "            model.add(DefaultConv2D(filters = f[c_counter]))\n",
    "            c_counter += 1\n",
    "        elif l == 'maxp':\n",
    "            model.add(layers.MaxPooling2D(pool_size = ps))\n",
    "            mp_counter += 1\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    for i in range(nlay):\n",
    "        model.add(layers.Dense(width[i], activation=act_fun[i]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0758d3f",
   "metadata": {},
   "source": [
    "The following cell shows an example of how we trained a number of CNN by varying one single hyperparameter (here the number of filters for each convolutional layer). The same process was applied for the other hyperparameters so the code would be very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73877f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [32, 64, 128, 256]  #number of filters that we want to study\n",
    "\n",
    "#first train the models on the fashion set\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) #define a stopping condition\n",
    "fashion_mnist = keras.datasets.fashion_mnist   #load up the data set\n",
    "X_train, Y_train, X_test, Y_test = load_data(fashion_mnist) #define a training and test set \n",
    "X_train, X_test = cnn_data(X_train, X_test)\n",
    "for i in F:\n",
    "    f = np.zeros(5) + i   #number of filters for each layer, note in this analysis we have all the conv. layers having the same number of filters\n",
    "    model = create_cnn(f = f) #create the model\n",
    "    print(model.summary())  \n",
    "    model = compile_model(model)   #compile the model\n",
    "    history = train_model(model, X_train, Y_train, num_epochs = 40)   #train the model\n",
    "    \n",
    "    #save the training history\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist_csv_file = 'cnn_fash_f={}.csv'.format(i)\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist.to_csv(f)\n",
    "    \n",
    "    #save the accuracy on the test set \n",
    "    test_acc = np.array(model.evaluate(X_test, Y_test))\n",
    "    test_name = 'cnn_fash_f={}.npy'.format(i)\n",
    "    with open(test_name, 'wb') as f:\n",
    "        np.save(f, test_acc)\n",
    "\n",
    "\n",
    "\n",
    "#repeat the same procedure for the digit set\n",
    "\n",
    "digit_mnist = keras.datasets.mnist \n",
    "X_train, Y_train, X_test, Y_test = load_data(digit_mnist)\n",
    "X_train, X_test = cnn_data(X_train, X_test)\n",
    "for i in F:\n",
    "    f = np.zeros(5) + i\n",
    "    model = create_cnn(f = f)\n",
    "    print(model.summary())\n",
    "    model = compile_model(model)\n",
    "    history = train_model(model, X_train, Y_train, num_epochs = 40)\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist_csv_file = 'cnn_f={}.csv'.format(i)\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist.to_csv(f)\n",
    "\n",
    "    test_acc = np.array(model.evaluate(X_test, Y_test))\n",
    "    test_name = 'cnn_f={}.npy'.format(i)\n",
    "    with open(test_name, 'wb') as f:\n",
    "        np.save(f, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
